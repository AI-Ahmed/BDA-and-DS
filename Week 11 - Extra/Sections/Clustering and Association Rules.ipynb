{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details style='font-size:16px'><summary style='font-size:22px'>Learning Objectives:</summary>\n",
    "\n",
    "- Understand different types of clustering alogrithms.\n",
    "\n",
    "- Apply clustering on different types of data sets.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/customer_segmentation.png\" style='float:right;'>\n",
    "\n",
    "**Customer Segmentation** is the practice of partitioning a customer base into groups of individuals that have similar characteristics.\n",
    "\n",
    "For example, one group might contain customers who are high profit and low risk. \n",
    "\n",
    "- That is, more likely to purchase products or subscribe for a service.\n",
    "\n",
    "- Knowing this information allows a business to devote more time and attention to retaining these customers.\n",
    "\n",
    "Another group might include customers from non-profit organization.\n",
    "\n",
    "Customers can be grouped based on several factors including `age`, `gender`, `interests`, ..etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to divide a set of customers are similar to each other..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/customer_segmentation_2.png\" style='float:right;'>\n",
    "\n",
    "**Clustering** can group data only <u>unsupervised</u>, based on the similarity of customers between each other. \n",
    "\n",
    "2. It will partition your customers into mutually exculsive groups. For example into <u>into three clusters</u>.\n",
    "\n",
    "*The customers in each cluster are similar to each other, demographically*.\n",
    "\n",
    "3. Let's create a profile for each group.\n",
    "\n",
    "4. Finally, we can assign each individual in our dataset to one of these groups or segments of customers.\n",
    "\n",
    "5. Now, imagine that you <u>cross join</u> this **segmented dataset** with the **actual dataset** of the products or services that customers purchase from your company.\n",
    "\n",
    "*This information would really help to understand and predict the difference and individual customers preferences and their buying behaviors across various products.*\n",
    "\n",
    "*Having this information will allow your company to develop highly personalized experiences for each segment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/cluster.png\" style='float:right;'>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "A Group of objects that are **similar to other objects** in cluster, \n",
    "\n",
    "and **dissimilar to data points** in other cluster. \n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Vs. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "<img src=\"Modules_images/classification_diff.png\" style='float:right;'>\n",
    "\n",
    "Classification algorithm <u>predict categorical classed labels.</u>\n",
    "\n",
    "This means assigning instances to pre-defined classes such as `Defaulted` or `not Defaulted`.\n",
    "\n",
    "For example, if an analyst wants to analyze customer data in order to know which customers might `default` on their payments, he/she uses a <u>labeled dataset</u> as *training data* and uses classification approaches such as **decision tree**, **Support Vector Machine (SVM)**, or **Logistic Regression** to <u>predict</u> the default value for a new or unknown customer.\n",
    "\n",
    "Generally speaking, **classification** is supervied learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "<img src=\"Modules_images/clustering_diff.png\" style='float:right;'>\n",
    "\n",
    "Clustering algorithm <u> the data **unlabeled** and the process is **unsupervised**</u>.\n",
    "\n",
    "For example, we can use a clustering algorithm such as **k-means** to group <u>similar customers and assign them to a cluster</u> Based on whether they share similar attribues, such as `age`, `education`, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering applications\n",
    "\n",
    "> Clustering is used to find association among customers based on their <u>demographic characteristics</u> and use that info. to identify buying patterns of various customers groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retail/ Marketing**:\n",
    "\n",
    "- Identifying buying patterns of customers. (e.g., *like in the big supermarket when they sort the items according to the customers items association.*)\n",
    "\n",
    "- Recommending new books or movies to new customers. (e.g., *Youtube, Netflix, Google books*)\n",
    "\n",
    "**Banking:**\n",
    "\n",
    "- Fraud detection in credit card use. (e.g., *normal transactions to find the patterns of fraudulent credit card usage.*)\n",
    "\n",
    "- Identifying clusters of customers (e.g., *loyal or churn*)\n",
    "\n",
    "**Insurance**:\n",
    "\n",
    "- Fraud detection in claims analysis.\n",
    "\n",
    "- Insurance rist of customers.\n",
    "\n",
    "**Publication:**\n",
    "\n",
    "- Auto-categorizing news based on their content.\n",
    "\n",
    "- Recommending similar news articles.\n",
    "\n",
    "**Medicine**:\n",
    "\n",
    "- Characterizing patient behavior (e.g., *Identify successful medical therapies for different illnesses*)\n",
    "\n",
    "**Biology**:\n",
    "\n",
    "- Clustering genetic markers to identify family ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploratory Data Analysis\n",
    "\n",
    "- Summary Generation\n",
    "\n",
    "- Outlier Detection (e.g., *especially to be used for fraud detection or noise removal*)\n",
    "\n",
    "- Finding Duplicates \n",
    "\n",
    "- Pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/partitioned_clustering.png\" style='float:right;'>\n",
    "<br><br>\n",
    "\n",
    "**Partitioned-based Clustering**: clustering alogrithms that produces <u>sphere-like cluster</u>.\n",
    "\n",
    "- Relatively efficient\n",
    "\n",
    "- E.g. **K-Means, K-Medians, or Fuzzy c-Means**\n",
    "\n",
    "- Dealling with <u>Medium-level</u> and <u>large-level</u> of dataset.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"Modules_images/Hier_clustering.png\" style='float:right;'>\n",
    "\n",
    "**Hierarchical Clustering**: \n",
    "\n",
    "- Produces trees of clusters\n",
    "\n",
    "- E.g. **Agglomerative, Divisive**\n",
    "\n",
    "- Dealling with <u>small-level</u> and <u>medium-level</u> of dataset.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"Modules_images/density_clustering.png\" style='float:right;'>\n",
    "\n",
    "**Density-based Clustering**: It's good when dealing with spatial clusters when there's noise in your dataset\n",
    "\n",
    "- Produces arbitrary shaped clusters.\n",
    "\n",
    "- E.g.  DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Introduction of K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/k-means.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "\n",
    "<span style='font-size:18px'>\n",
    "In customer dataset;\n",
    "\n",
    "\n",
    "Apply customer segmentation on this historical data.\n",
    "\n",
    "**Custoemr Segmentation** is the practice of <u>partitioning a customer</u> base into groups of individuals that have similar characteristics.\n",
    "\n",
    "**K-means** can group only <u>unsupervised</u> based on the similarity of customer to each other.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/k-means_clustering.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "- Partitioning Clustering\n",
    "\n",
    "- K-means divides the data into **non-overlapping** subsets (clusters) without any cluster-internal structure.\n",
    "\n",
    "- Examples within a clusters are <u>very similar</u>.\n",
    "\n",
    "- Examples across different clusters are <u>very different</u>.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/sim_dissim.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "\n",
    "**K-means** tries to <u>minimize</u> *Intra-cluster* distance between the simples, inside cluster, and <u>maximize</u> *Inter-cluster* between the clusters, outside.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we can measure the distance using **Euclidean Distance** as an example of K-mean measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_1.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_2.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_3.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm\n",
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "1. Initialize **k=3** Randomly selected.\n",
    "</span>\n",
    "\n",
    "    - $C_1 = [5., 5.]$\n",
    "    - $C_2 = [8., 5.]$\n",
    "    - $C_3 = [6., 3.]$\n",
    "\n",
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "2. Calculate the Distance: </b></span>\n",
    "    \n",
    "    + between each customer and these initialized points. Use different measures of distance may be used to place items into clusters. Therefore, you will form a <u>matrix</u> where each row represents the <u>distance of a customer for each centroid</u> which is called **Distance Matrix**.\n",
    "\n",
    "<img src=\"Modules_images/k-means_cal_2.png\" style='float:right; width:950px;height:489px'>\n",
    "<img src=\"Modules_images/k-means_cal_1.png\" style='float:left; width: 750px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "3. Assign each point to the closeset centroid.\n",
    "</span>\n",
    "\n",
    "    - The main objective of **K-Means** clustering is to <u>minimize</u> the distance of data points from the centriod of the cluster and <u>maximize</u> the distance from other centroids.\n",
    "\n",
    "    - We can easily say that it doesn't result in a good cluster because the centroids were chosen randomly from the first. So that, the model have high error.\n",
    "    \n",
    "    - **Sum of squares Error (SSE)**: is the total distance of each point from its centroid.\n",
    "\n",
    "Now, the question is, **how can we turn it into better clusters with <u>less error</u>?**\n",
    "    \n",
    "<center>\n",
    "<img src=\"Modules_images/k-means_cal_3.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "4. Compute the new centroids for each cluster.\n",
    "</span>\n",
    "\n",
    "- First, each cluster will be updated to be in the **mean coordinates** for datapoints in its cluster. In other word, the centroids of each of the three clusters becomes the new mean.\n",
    "\n",
    "- For example; $C_2$ will be in the mean of point `A(7.4, 3.6)` and `B(7.8, 3.8)` which is $C_2(7.6, 3.7) \\to \\frac{x_1 + x_2}{n}\\; |\\;  \\frac{y_1 + y_2}{n}$\n",
    "<center>\n",
    "<img src=\"Modules_images/k-means_cal_4.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "5. Repeat until there're no more changes.\n",
    "</span>\n",
    "\n",
    "- This continuous until the centroids no longer move.\n",
    "\n",
    "- K-Means is iterative algorithm and we have to repeat the steps 2-4 times until the algorithm converges.\n",
    "\n",
    "- In the results in the clusters with minimum error or the most dense cluster. However, as it is a **heuristic alogrithm**, There's no guarantee that it will converge to the global optimum and the result may depend on the intial cluster.\n",
    "\n",
    "- It means, this algorithm is guaranteed to converge to a result, but the result may be local optimum not necessarily the best possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/K-mean_algorithm.gif\">\n",
    "</center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, it's common to run the whole process multiple times with different starting conditions.\n",
    "\n",
    "This means with randomized starting centroids, it may give a better outcome. \n",
    "    \n",
    "As an algorithm, is usually **very fast**. It wouldn't be any problem to run it multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) More on k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recaping The k-Means Algorithm\n",
    "\n",
    "1. Randomly placing *k* centroids, one for each cluster. *The futher apart the clusters are placed, the better*.\n",
    "\n",
    "2. Calculate the distance of each point from each centroid.\n",
    "\n",
    "3. Assign each data point (object) to its closest centroid, creating a cluster.\n",
    "\n",
    "4. Recalculate the position of the *k* centroids.\n",
    "\n",
    "5. Repeat the steps 2-4 times, until the centroids no longer move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important question is **How can we evaluate the goodness of the clusters formed by k-Means?** or **How to caculate the accuracy of the k-Means?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Accuaracy\n",
    "\n",
    "We have two ways to calculate the accuracy of the k-Means\n",
    "\n",
    "<img src=\"Modules_images/k-means_clustering.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "\n",
    "**External approach** (*Inter-cluster Approach*)\n",
    "\n",
    "+ Compare the cluster with the ground truth. (*since k-Means is an <u>unsupervied algorithm</u> we usually don't have ground truth in the real world problems to be used.*)\n",
    "\n",
    "**Internal approach** (*Intra-cluster Approach*)\n",
    "\n",
    "+ Average the distance between the data points within the cluster. (*it can be used as metric of error for the clustering algorithm*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct *k* is often ambiguous because it's very dependent on the shape and scale of the distribution of points in a dataset.\n",
    "\n",
    "One of the techniques that is commonly used is to run the clustering across the different values of *k* and looking at a metric of accuracy for clustering.\n",
    "\n",
    "The metric can be <u>mean distance between data points and their cluster centroid</u> Which indicate how dense our clusters are, or to what extent we <u>minimize the error of clustering.</u>\n",
    "\n",
    "Then looking at the change of this metric, we can find the best value of *k*.\n",
    "\n",
    "But, the problem is that <u>with increasing the number of clusters, the distance of centroids to the data points will always reduce.<u>\n",
    "\n",
    "This means **increasing the value of *k*, will always decreasing the error**\n",
    "\n",
    "The **elbow point** that in the plot determines the rate of decrease sharply shifts. and <u>it is the right K for clustering</u> This method called **Elbow Method**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/elbow_method.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means recap\n",
    "\n",
    "- k-Means is a <u>unsupervised partitioned-based clustering</u> which is relatively efficient on <u>medium</u> and <u>large</u> dataset.\n",
    "\n",
    "- Produces <u>sphere</u>-like clusters. Because the clusters shaped around the centroids.\n",
    "\n",
    "- we should pre-specify the numbers of clusters which is really one of <u>drawback</u> (*Not simple process*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Association Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rules are a part of data mining that reveal relationships between variables in large datasets. They are commonly used in market basket analysis, where the goal is to find associations between products that are frequently bought together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rules Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rules Mining is a technique in data mining that helps us discover interesting relationships or associations among different variables in large datasets. The primary goal is to identify patterns where the occurrence of one event is linked to the occurrence of another.\n",
    "\n",
    "Imagine you have a supermarket transaction dataset. Association Rules Mining could reveal associations like \"Customers who buy bread often also buy butter\" or \"People purchasing diapers are likely to buy baby formula.\" These associations are valuable for businesses as they provide insights into customer behavior and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components\n",
    "\n",
    "1. **Items:** These are the products or variables involved in the association. In a supermarket example, items could be specific products like bread, butter, diapers, and baby formula.\n",
    "\n",
    "2. **Transactions:** A transaction is a set of items purchased together. For instance, the items bought by a single customer in one shopping trip.\n",
    "\n",
    "3. **Support:** This measures how frequently an association (a set of items) appears in the dataset. High support indicates a strong association.\n",
    "\n",
    "4. **Confidence:** This indicates the likelihood that an item B is purchased when item A is purchased. It is calculated as the support for both items (A and B together) divided by the support for item A.\n",
    "\n",
    "5. **Lift:** Lift measures how much more likely item B is purchased when item A is purchased compared to when B is purchased without A. It is calculated by dividing the confidence of the rule by the support of item B.\n",
    "\n",
    "Mathematically, these are represented as follows:\n",
    "\n",
    "$$\\text{Support}(A \\rightarrow B) = \\frac{\\text{Transactions containing both A and B}}{\\text{Total transactions}}$$\n",
    "\n",
    "$$\\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cap B)}{\\text{Support}(A)}$$\n",
    "\n",
    "$$\\text{Lift}(A \\rightarrow B) = \\frac{\\text{Confidence}(A \\rightarrow B)}{\\text{Support}(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a classic algorithm for association rule mining. It's widely used for discovering interesting relationships between items in large datasets. The algorithm works based on the concept of \"apriori property,\" which states that if an itemset is frequent, then all of its subsets must also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Generate Candidate Itemsets:**\n",
    "   - Start with frequent itemsets of length 1.\n",
    "   - Generate candidate itemsets of length $k + 1$ from frequent itemsets of length $k$.\n",
    "\n",
    "2. **Prune Candidate Itemsets:**\n",
    "   - Remove candidate itemsets that contain infrequent subsets.\n",
    "\n",
    "3. **Calculate Support:**\n",
    "   - Count the support (occurrences) of each candidate itemset.\n",
    "\n",
    "4. **Generate Frequent Itemsets:**\n",
    "   - Select itemsets with support greater than a predefined threshold as frequent itemsets.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Repeat the process until no more frequent itemsets can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Credit Transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a dataset of credit transactions where each transaction represents a person's credit record. The items could be different credit-related attributes like \"High Income,\" \"Good Credit Score,\" \"Owns a Home,\" and \"Approved for a Loan.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you may want to intall `mlxtend` library for Apriori algorithm:\n",
    "```python\n",
    "!pip install mlxtend\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will generate 1000 records of transcation to simulate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate random values for the dataset\n",
    "def generate_random_data(size):\n",
    "    income_levels = np.random.choice(['High', 'Medium', 'Low'], size=size)\n",
    "    credit_scores = np.random.choice(['Good', 'Excellent', 'Fair'], size=size)\n",
    "    home_ownership = np.random.choice(['Owns', 'Rents'], size=size)\n",
    "    loan_approval = np.random.choice(['Approved', 'Not Approved'], size=size)\n",
    "\n",
    "    data = {\n",
    "        'Income Level': income_levels,\n",
    "        'Credit Score': credit_scores,\n",
    "        'Home Ownership': home_ownership,\n",
    "        'Loan Approval': loan_approval,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate 1000 records\n",
    "dataset_size = 1000\n",
    "credit_records = generate_random_data(dataset_size)\n",
    "\n",
    "# Display the first few records of the generated dataset\n",
    "credit_records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to work on this categorical variables, we need to encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert dataset to one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(credit_records).transform(credit_records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the one-hot encoded DataFrame\n",
    "print(\"\\nOne-Hot Encoded DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can model the Apriori to obtain the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "\n",
    "\n",
    "# Display frequent itemsets and association rules\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you generate association rules using the `association_rules` function from the `mlxtend` library, the resulting DataFrame (`rules` in this case) already contains columns for support, confidence, lift, and leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the `min_support` parameter sets the minimum support threshold, and the `min_threshold` parameter in `association_rules` sets the minimum confidence threshold for generating rules. The resulting output will show you the frequent itemsets and association rules discovered from the credit records dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2 style='color:cyan'>JUST FOR FUN :D</h2>\n",
    "<video controls\n",
    "   src=\"https://archive.org/download/BigBuckBunny_124/Content/big_buck_bunny_720p_surround.mp4\"\n",
    "    poster=\"https://peach.blender.org/wp-content/uploads/title_anouncement.jpg?x11217\" \n",
    "     width= '1080px'></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
