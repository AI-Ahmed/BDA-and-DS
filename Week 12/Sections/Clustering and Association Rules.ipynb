{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details style='font-size:16px'><summary style='font-size:22px'>Learning Objectives:</summary>\n",
    "\n",
    "- Understand different types of clustering alogrithms.\n",
    "\n",
    "- Apply clustering on different types of data sets.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/customer_segmentation.png\" style='float:right;'>\n",
    "\n",
    "**Customer Segmentation** is the practice of partitioning a customer base into groups of individuals that have similar characteristics.\n",
    "\n",
    "For example, one group might contain customers who are high profit and low risk. \n",
    "\n",
    "- That is, more likely to purchase products or subscribe for a service.\n",
    "\n",
    "- Knowing this information allows a business to devote more time and attention to retaining these customers.\n",
    "\n",
    "Another group might include customers from non-profit organization.\n",
    "\n",
    "Customers can be grouped based on several factors including `age`, `gender`, `interests`, ..etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to divide a set of customers are similar to each other..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/customer_segmentation_2.png\" style='float:right;'>\n",
    "\n",
    "**Clustering** can group data only <u>unsupervised</u>, based on the similarity of customers between each other. \n",
    "\n",
    "2. It will partition your customers into mutually exculsive groups. For example into <u>into three clusters</u>.\n",
    "\n",
    "*The customers in each cluster are similar to each other, demographically*.\n",
    "\n",
    "3. Let's create a profile for each group.\n",
    "\n",
    "4. Finally, we can assign each individual in our dataset to one of these groups or segments of customers.\n",
    "\n",
    "5. Now, imagine that you <u>cross join</u> this **segmented dataset** with the **actual dataset** of the products or services that customers purchase from your company.\n",
    "\n",
    "*This information would really help to understand and predict the difference and individual customers preferences and their buying behaviors across various products.*\n",
    "\n",
    "*Having this information will allow your company to develop highly personalized experiences for each segment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/cluster.png\" style='float:right;'>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "A Group of objects that are **similar to other objects** in cluster, \n",
    "\n",
    "and **dissimilar to data points** in other cluster. \n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Vs. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "<img src=\"Modules_images/classification_diff.png\" style='float:right;'>\n",
    "\n",
    "Classification algorithm <u>predict categorical classed labels.</u>\n",
    "\n",
    "This means assigning instances to pre-defined classes such as `Defaulted` or `not Defaulted`.\n",
    "\n",
    "For example, if an analyst wants to analyze customer data in order to know which customers might `default` on their payments, he/she uses a <u>labeled dataset</u> as *training data* and uses classification approaches such as **decision tree**, **Support Vector Machine (SVM)**, or **Logistic Regression** to <u>predict</u> the default value for a new or unknown customer.\n",
    "\n",
    "Generally speaking, **classification** is supervied learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "<img src=\"Modules_images/clustering_diff.png\" style='float:right;'>\n",
    "\n",
    "Clustering algorithm <u> the data **unlabeled** and the process is **unsupervised**</u>.\n",
    "\n",
    "For example, we can use a clustering algorithm such as **k-means** to group <u>similar customers and assign them to a cluster</u> Based on whether they share similar attribues, such as `age`, `education`, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering applications\n",
    "\n",
    "> Clustering is used to find association among customers based on their <u>demographic characteristics</u> and use that info. to identify buying patterns of various customers groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retail/ Marketing**:\n",
    "\n",
    "- Identifying buying patterns of customers. (e.g., *like in the big supermarket when they sort the items according to the customers items association.*)\n",
    "\n",
    "- Recommending new books or movies to new customers. (e.g., *Youtube, Netflix, Google books*)\n",
    "\n",
    "**Banking:**\n",
    "\n",
    "- Fraud detection in credit card use. (e.g., *normal transactions to find the patterns of fraudulent credit card usage.*)\n",
    "\n",
    "- Identifying clusters of customers (e.g., *loyal or churn*)\n",
    "\n",
    "**Insurance**:\n",
    "\n",
    "- Fraud detection in claims analysis.\n",
    "\n",
    "- Insurance rist of customers.\n",
    "\n",
    "**Publication:**\n",
    "\n",
    "- Auto-categorizing news based on their content.\n",
    "\n",
    "- Recommending similar news articles.\n",
    "\n",
    "**Medicine**:\n",
    "\n",
    "- Characterizing patient behavior (e.g., *Identify successful medical therapies for different illnesses*)\n",
    "\n",
    "**Biology**:\n",
    "\n",
    "- Clustering genetic markers to identify family ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploratory Data Analysis\n",
    "\n",
    "- Summary Generation\n",
    "\n",
    "- Outlier Detection (e.g., *especially to be used for fraud detection or noise removal*)\n",
    "\n",
    "- Finding Duplicates \n",
    "\n",
    "- Pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/partitioned_clustering.png\" style='float:right;'>\n",
    "<br><br>\n",
    "\n",
    "**Partitioned-based Clustering**: clustering alogrithms that produces <u>sphere-like cluster</u>.\n",
    "\n",
    "- Relatively efficient\n",
    "\n",
    "- E.g. **K-Means, K-Medians, or Fuzzy c-Means**\n",
    "\n",
    "- Dealling with <u>Medium-level</u> and <u>large-level</u> of dataset.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"Modules_images/Hier_clustering.png\" style='float:right;'>\n",
    "\n",
    "**Hierarchical Clustering**: \n",
    "\n",
    "- Produces trees of clusters\n",
    "\n",
    "- E.g. **Agglomerative, Divisive**\n",
    "\n",
    "- Dealling with <u>small-level</u> and <u>medium-level</u> of dataset.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"Modules_images/density_clustering.png\" style='float:right;'>\n",
    "\n",
    "**Density-based Clustering**: It's good when dealing with spatial clusters when there's noise in your dataset\n",
    "\n",
    "- Produces arbitrary shaped clusters.\n",
    "\n",
    "- E.g.  DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Introduction of K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/k-means.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "\n",
    "<span style='font-size:18px'>\n",
    "In customer dataset;\n",
    "\n",
    "\n",
    "Apply customer segmentation on this historical data.\n",
    "\n",
    "**Custoemr Segmentation** is the practice of <u>partitioning a customer</u> base into groups of individuals that have similar characteristics.\n",
    "\n",
    "**K-means** can group only <u>unsupervised</u> based on the similarity of customer to each other.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/k-means_clustering.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "- Partitioning Clustering\n",
    "\n",
    "- K-means divides the data into **non-overlapping** subsets (clusters) without any cluster-internal structure.\n",
    "\n",
    "- Examples within a clusters are <u>very similar</u>.\n",
    "\n",
    "- Examples across different clusters are <u>very different</u>.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/sim_dissim.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "\n",
    "**K-means** tries to <u>minimize</u> *Intra-cluster* distance between the simples, inside cluster, and <u>maximize</u> *Inter-cluster* between the clusters, outside.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we can measure the distance using **Euclidean Distance** as an example of K-mean measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_1.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_2.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Dimensional similarity/Distance\n",
    "<center>\n",
    "<img src=\"Modules_images/euclidean_3.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm\n",
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "1. Initialize **k=3** Randomly selected.\n",
    "</span>\n",
    "\n",
    "    - $C_1 = [5., 5.]$\n",
    "    - $C_2 = [8., 5.]$\n",
    "    - $C_3 = [6., 3.]$\n",
    "\n",
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "2. Calculate the Distance: </b></span>\n",
    "    \n",
    "    + between each customer and these initialized points. Use different measures of distance may be used to place items into clusters. Therefore, you will form a <u>matrix</u> where each row represents the <u>distance of a customer for each centroid</u> which is called **Distance Matrix**.\n",
    "\n",
    "<img src=\"Modules_images/k-means_cal_2.png\" style='float:right; width:950px;height:489px'>\n",
    "<img src=\"Modules_images/k-means_cal_1.png\" style='float:left; width: 750px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "3. Assign each point to the closeset centroid.\n",
    "</span>\n",
    "\n",
    "    - The main objective of **K-Means** clustering is to <u>minimize</u> the distance of data points from the centriod of the cluster and <u>maximize</u> the distance from other centroids.\n",
    "\n",
    "    - We can easily say that it doesn't result in a good cluster because the centroids were chosen randomly from the first. So that, the model have high error.\n",
    "    \n",
    "    - **Sum of squares Error (SSE)**: is the total distance of each point from its centroid.\n",
    "\n",
    "Now, the question is, **how can we turn it into better clusters with <u>less error</u>?**\n",
    "    \n",
    "<center>\n",
    "<img src=\"Modules_images/k-means_cal_3.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "4. Compute the new centroids for each cluster.\n",
    "</span>\n",
    "\n",
    "- First, each cluster will be updated to be in the **mean coordinates** for datapoints in its cluster. In other word, the centroids of each of the three clusters becomes the new mean.\n",
    "\n",
    "- For example; $C_2$ will be in the mean of point `A(7.4, 3.6)` and `B(7.8, 3.8)` which is $C_2(7.6, 3.7) \\to \\frac{x_1 + x_2}{n}\\; |\\;  \\frac{y_1 + y_2}{n}$\n",
    "<center>\n",
    "<img src=\"Modules_images/k-means_cal_4.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:cyan;font-size:17px'>\n",
    "<b>\n",
    "\n",
    "5. Repeat until there're no more changes.\n",
    "</span>\n",
    "\n",
    "- This continuous until the centroids no longer move.\n",
    "\n",
    "- K-Means is iterative algorithm and we have to repeat the steps 2-4 times until the algorithm converges.\n",
    "\n",
    "- In the results in the clusters with minimum error or the most dense cluster. However, as it is a **heuristic alogrithm**, There's no guarantee that it will converge to the global optimum and the result may depend on the intial cluster.\n",
    "\n",
    "- It means, this algorithm is guaranteed to converge to a result, but the result may be local optimum not necessarily the best possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/K-mean_algorithm.gif\">\n",
    "</center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, it's common to run the whole process multiple times with different starting conditions.\n",
    "\n",
    "This means with randomized starting centroids, it may give a better outcome. \n",
    "    \n",
    "As an algorithm, is usually **very fast**. It wouldn't be any problem to run it multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) More on k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recaping The k-Means Algorithm\n",
    "\n",
    "1. Randomly placing *k* centroids, one for each cluster. *The futher apart the clusters are placed, the better*.\n",
    "\n",
    "2. Calculate the distance of each point from each centroid.\n",
    "\n",
    "3. Assign each data point (object) to its closest centroid, creating a cluster.\n",
    "\n",
    "4. Recalculate the position of the *k* centroids.\n",
    "\n",
    "5. Repeat the steps 2-4 times, until the centroids no longer move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important question is **How can we evaluate the goodness of the clusters formed by k-Means?** or **How to caculate the accuracy of the k-Means?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Accuaracy\n",
    "\n",
    "We have two ways to calculate the accuracy of the k-Means\n",
    "\n",
    "<img src=\"Modules_images/k-means_clustering.png\" style='float:right;'>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "\n",
    "**External approach** (*Inter-cluster Approach*)\n",
    "\n",
    "+ Compare the cluster with the ground truth. (*since k-Means is an <u>unsupervied algorithm</u> we usually don't have ground truth in the real world problems to be used.*)\n",
    "\n",
    "**Internal approach** (*Intra-cluster Approach*)\n",
    "\n",
    "+ Average the distance between the data points within the cluster. (*it can be used as metric of error for the clustering algorithm*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct *k* is often ambiguous because it's very dependent on the shape and scale of the distribution of points in a dataset.\n",
    "\n",
    "One of the techniques that is commonly used is to run the clustering across the different values of *k* and looking at a metric of accuracy for clustering.\n",
    "\n",
    "The metric can be <u>mean distance between data points and their cluster centroid</u> Which indicate how dense our clusters are, or to what extent we <u>minimize the error of clustering.</u>\n",
    "\n",
    "Then looking at the change of this metric, we can find the best value of *k*.\n",
    "\n",
    "But, the problem is that <u>with increasing the number of clusters, the distance of centroids to the data points will always reduce.<u>\n",
    "\n",
    "This means **increasing the value of *k*, will always decreasing the error**\n",
    "\n",
    "The **elbow point** that in the plot determines the rate of decrease sharply shifts. and <u>it is the right K for clustering</u> This method called **Elbow Method**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/elbow_method.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means recap\n",
    "\n",
    "- k-Means is a <u>unsupervised partitioned-based clustering</u> which is relatively efficient on <u>medium</u> and <u>large</u> dataset.\n",
    "\n",
    "- Produces <u>sphere</u>-like clusters. Because the clusters shaped around the centroids.\n",
    "\n",
    "- we should pre-specify the numbers of clusters which is really one of <u>drawback</u> (*Not simple process*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Intro. to Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/hierarchical_clustering.png\" style='float:right;'>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<span style='font-size:21px'>\n",
    "\n",
    "**Hierarchical Clustering algorithm** build a hierarchy of clusters where <u>each node is a cluster</u> consists of the clusters of its daughter nodes.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Types\n",
    "\n",
    "There're **two types** of hierarchical clustering:\n",
    "\n",
    "- Divisive\n",
    "\n",
    "- Agglomerative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/hier_types_2.png\" style='float:left;width:600px;height:365px'>\n",
    "<img src=\"Modules_images/hier_types_1.png\" style='float:left;width:500px;height:365px'>\n",
    "<img src=\"Modules_images/hier_types_3.png\" style='float:right;width:600px;height:365px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divisive**\n",
    "\n",
    " - <u>Top-down</u>, so you can start with all observations (*rows*)  in a <u>large cluster</u> and break them into <u>smaller pieces cluster</u>. Think about divisive as dividing the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agglomerative**\n",
    "\n",
    " - <u>Bottom-up</u>, where each observations (*rows*)  starts with its own cluster and pair of clusters are merged togther as they move up to the hierarchy. Agglomerative means <u>amass or collect things</u> which exactly does with the cluster. It also more popular between data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Algorithm\n",
    "\n",
    "In our example, let's say we want to cluster six cities in Canada based on their <u>distances from one another.</u>\n",
    "\n",
    "They're Toronto, Ottawa, Vancouver, Montreal, Winnipeg, and Edmonton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/agglo_1.png\" style='float:left;width:850px;'>\n",
    "<img src=\"Modules_images/agglo_2.png\" style='float:right;width:850px;height:333px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is started by assigning each city to its own cluster. which mean we have <u>6 Clusters</u> each containing just <u>1 Cluster</u>.\n",
    "\n",
    "Usually we want to take <u>two closest clusters</u> according to the chosen distance.\n",
    "\n",
    "From the second picture we can see that `Montreal` and `Ottawa` are the closest clusters so we make cluster out of them. Please notice that we just use a **simple one-dimensional distance** feature here, but our object can be **multidimensional and distance measurement** can either be **Euclidean**, **Pearson**, **average distance** or many others depending on data type and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, we can see in the distance matrix, rows and columns related to `MO` and `OT` citites are <u>merged</u> as the cluster is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/hier_clustering_4.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the distance from all the cities to this new <u>merged cluster</u> get updated, but how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/hier_clustering_5.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we calculate the distacne between `WI` to `OT/MO`\n",
    "\n",
    "But let's assume that we selected the distance from the center of `OT/MO` cluster to `WI`.\n",
    "\n",
    "Updating the distance matrix, we now have one less cluster. We have one less cluster (*which means that you don't need to go from `Ottawa` to `Montreal` then go to `Winnipeg`*).\n",
    "\n",
    "Example here, Before making the seperation, we can notice that the  travel  `Montreal` to `Winnipeg` = *1824*, even the way from `Montreal/Ottawa` to `Winnipeg` = *167+1676 = 1843*. Which mean it is higher than the number before the seperation.\n",
    "\n",
    "After the Agglomerative it is now `Montreal/Ottawa` to `Winnipeg` = *1676*\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, We check the next <u>closest one to create a new cluster</u>, which `OT/MO` and `TO` \n",
    "\n",
    "<span style='color:red'>**Notice**:</span> Everytime we merge the columns and rows, beside choosing the lowest value in the distance metrix, we also merge their data by <u>choosing the lowest upper observed tuple comparing with the feature (column) we're going to merge with</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/hier_clustering_6.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat until all the clusters are merged and the tree becomes completed\n",
    "<center>\n",
    "<img src=\"Modules_images/hier_clustering_7.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/dendrogram.png\" style='float:right;'>\n",
    "\n",
    "Hierarchical Clustering are typically visuallized as **Dendrogram**.\n",
    "\n",
    "Each merge represented by a <u>horizontal line</u>.\n",
    "\n",
    "The `y-coordinate` of the horizontal line is <u>similarity of two clusters that were merged where cities are viewed as singleton (unique pattern) cluster</u>.\n",
    "\n",
    "By moving up from the bottom layer to the top node, <u>dendrogram allows us to reconstruct the history of merges that resulted in the depicked (drawn) clustering.</u>\n",
    "\n",
    "Essentially, Hierarchical clustering does not require a <u>prespecified number of clusters</u>. (*e.g. k-Means, k-median, c-fuzzy mean*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/hier_clustering_8.png\" style='float:right;'>\n",
    "\n",
    "However, in some applications, we want to <u>partition of disjoint</u> clusters just as in float clustering.\n",
    "\n",
    "\n",
    "In this situation, the hierarchy need to cut at some points.\n",
    "\n",
    "Example here, cutting in a specific level of similarity, we create <u>3 clusters of similar citites</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recaping The Agglomerative Algorithm \n",
    "<center>\n",
    "<video controls autorepeat\n",
    "    src=\"videos/hierarchical_clustering_algorithm.mp4\">\n",
    "<\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E) Evaluation of Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/agglo_3.png\" style='float:right;'>\n",
    "<br><br>\n",
    "<br><br>\n",
    "\n",
    "<span style='font-size:18px'>\n",
    "\n",
    "1. Create *n*  clusters, one for each data point. (*using one of the statistical distance algorithm like  Euclidean Distance*)\n",
    "\n",
    "2. Compute the **Proximity Matrix**.\n",
    "\n",
    "3. **Repeat**\n",
    "\n",
    "    - Merge to closest clusters.\n",
    "    - Update the proximity matrix.\n",
    "\n",
    "\n",
    "4. **Until** only a <u>single cluster remains.</u>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there are a number of key questions that need to be answered.\n",
    "\n",
    "For instance, \n",
    "<span style='font-size:18px'>\n",
    "\n",
    "- **how do we <u>measure the distances</u> between these clusters?** \n",
    "                                                                                                                                                                                                                                                                                \n",
    "                            IMPORNTANT\n",
    "    \n",
    "- **How do we define <u>the nearest among clusters</u>?**. We also can ask **Which point do we use?**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating **how do we measure the distances between these clustes?** \n",
    "\n",
    "Let's assume that we have a dataset of patients and we want to cluster them using **hierarchy clustering**.\n",
    "\n",
    "We can use different distance measurements to calculate the <u>ploximity matrix</u>. For instance, **Euclidean Distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/proximity_matrix_euclidean.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we have a **Dataset** of *n* patience, we can build an *n*  **dissimilarity distance matrix**.\n",
    "\n",
    "It will give us the distance of clusters with one data point.\n",
    "\n",
    "However, as we mentioned, we merge clusters in **agglomerative clustering**.\n",
    "\n",
    "Now the question is, **how can we calculate the distance between clusters when there are builtible patients in each cluster?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/proximity_matrix_euclidean_2.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/distance_between_clusters.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "- Doesn't required number of clusters to be specified. (*e.g. for instance, k-Means requires to specify the number of clusters at the beginning*.)\n",
    "\n",
    "- Easy to implement.\n",
    "\n",
    "- Produces a *dendrogram*, which helps with understanding data.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Can never undo any previous steps throughout the algorithm. (*e.g., the algorithm clusters two points and later on, we see that the condition was not good one. <u>The program can not undo that step</u>*)\n",
    "\n",
    "- Generally has long runtimes. (*e.g., the time complexity for the clustering can result in very long computation times in comparison with efficient algorithm such as <u>k-Means</u>*) \n",
    "\n",
    "- Sometimes difficult to identify the correct numbers of clusters by the dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Vs. k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/hier_vs_k-means.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (F) Density-Based Clustering (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A density-based clustering algorithm which is approprate to use when examining spatial data. (*e.g., the data or the information that identifies geographic locations of features or boundaries*).\n",
    "\n",
    "Most of the traditional clustering techniques such as <u>k-Means</u>, <u>hierarchical</u>, and <u>fuzzy</u> clustering can be used to group data in an **unsupervised way**. (**Spherical-shape clusters**)\n",
    "\n",
    "However, when applied to tasks with **Arbitrary-shape clusters** or <u>clusters within clusters</u> the traditional techniques might not share enough similarity or the preformance may be <u>poor</u>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"Modules_images/Density-based_clustering.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means vs. Density-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**k-Means**\n",
    "\n",
    "In results of <u>anomalous points</u> (*points that is deviating from what is standard, normal, or expected*) this cause problems will assigned to the same clusters as normal data points. The anomlous points pull the cluster centroids towards. \n",
    "<br><br>\n",
    "**Density-Based Clustering**\n",
    "\n",
    "In contrast, density-based clustering locates regions of <u>high density that are separated</u> from <u>one another by regions of low density</u>.\n",
    "\n",
    "In this context is defined as the number of points within a specified radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/Density-based_clustering_2.png\" width='1150px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN For Class identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a particularly effective for tasks like class identifications on <u>spatial context</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/DBSCAN_1.png\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can find out any arbitary shaped clusters without getting effected by <u>noise</u>. (*e.g. Like this map that display the weather in canda*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/DBSCAN_2.png\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is DBSCAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise)\n",
    "\n",
    "- Is one of the most common clustering algorithm.\n",
    "- Works based on <u>density of objects</u>.\n",
    "<br><br>\n",
    "**There're two aspects for DBSCAN**\n",
    "<br><br><br>\n",
    "<img src=\"Modules_images/radius.png\" style='float:right;'>\n",
    "\n",
    "\n",
    "***R (Radius of neighborhood)***\n",
    "\n",
    "- Radius (R) that if includes enough number of points within, <u>we call it a dense area</u>.\n",
    "<br><br><br><br><br><br><br>\n",
    "<img src=\"Modules_images/min.png\" style='float:right;'>\n",
    "\n",
    "***M (Minimum number of neighbors)***\n",
    "\n",
    "- The minimum number of data points we want in a neighborhood to define a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How DSCAN works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_work.png\" style='float:right;'>\n",
    "<br>\n",
    "To see how DBSCAN works, we have to determine the type of points. \n",
    "<br><br><br><br>\n",
    "Each point in our dataset can be either a;\n",
    "\n",
    "1. Core point\n",
    "\n",
    "2. Border point\n",
    "\n",
    "3. Outlier Point\n",
    "\n",
    "But first, lets determines specific\n",
    "\n",
    "**R** (radius) that if it inlcludes enough points within it, we can call it <u>dense area</u>. (*e.g. radius= 2cm for sake of simplicity*)\n",
    "\n",
    "**M** determines the <u>minimum numbers of data points we want in a neighborhood to define a cluster</u>. (*e.g. minimum_number_of_points= 6 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Core point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_core.png\" style='float:right;'>\n",
    "<br>\n",
    "\n",
    "Now, let's pick a point, randomly. But first let's see whether it's a <span style='color:red'>core point</span> or not. (*You can find that by applying 2 conditions that we previously talked about -> [radius = 2cm, total_points_into_radius_range >= 6 points]*)\n",
    "<br><br>\n",
    "**What's core point?**\n",
    "<br>\n",
    "- A data point is a core point if within our neighborhood of the points there are at least *M* points (*e.g., M>=6*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Border point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_border.png\" style='float:right;'>\n",
    "<br>\n",
    "As you can see, there's only <u>five points</u> in this neighborhood inluding the <span style='color:yellow'>yellow point</span>\n",
    "\n",
    "<br><br>\n",
    "**What's border point?**\n",
    "<br>\n",
    "- A data point is a border point if **A**; its neighbourhood contains less than *M* data point or **B**. \n",
    "\n",
    "- It is reachable from some core point.\n",
    "\n",
    "- Here, reachability means it is within our distance from a <u>core point</u>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_core_2.png\" style='float:right;'>\n",
    "<br><br>\n",
    "\n",
    "<span style='font-size:20px'>\n",
    "Let's continue to the other points.\n",
    "\n",
    "As you can see, it also a <u>core point</u>.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Outlier Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_outliers.png\" style='float:right;'>\n",
    "<br>\n",
    "\n",
    "Let's pick this next point,\n",
    "\n",
    "\n",
    "you can see, it's not a <u>core point</u> not is it a <u>border point</u>.\n",
    "\n",
    "So, we'd label is as an <u><span style='color:gray'>outlier point</span></u>.\n",
    "\n",
    "<br><br>\n",
    "**What's outlier point?**\n",
    "<br>\n",
    "- It is not a point that is not a core point and also is not close enough to be reachable from a core point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_core_3.png\" style='float:right;'>\n",
    "<br><br>\n",
    "We continue and visit all the points in the dataset and label them as either <u>core</u>, <u>border</u>, or <u>outlier</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Modules_images/DBSCAN_cluster.png\" style='float:right;'>\n",
    "<br><br>\n",
    "The next step is to connect the core points that are neighbors and put them into the same <u>cluster</u>.\n",
    "\n",
    "**Cluster** is formated as <u>at least one *core point* + all *reachable core points* + all their *borders*</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"Modules_images/DBSCAN_adv.png\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2 style='color:cyan'>JUST FOR FUN :D</h2>\n",
    "<video controls\n",
    "   src=\"https://archive.org/download/BigBuckBunny_124/Content/big_buck_bunny_720p_surround.mp4\"\n",
    "    poster=\"https://peach.blender.org/wp-content/uploads/title_anouncement.jpg?x11217\" \n",
    "     width= '1080px'></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
